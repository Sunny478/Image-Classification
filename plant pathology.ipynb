{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "import tensorflow as tf, tensorflow.keras.backend as K\n",
    "from tensorflow.keras import models\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "!pip install -q efficientnet\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "main_path = '../input/plant-pathology-2020-fgvc7/'\n",
    "train_df = pd.read_csv(os.path.join(main_path,'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(main_path,'test.csv'))\n",
    "sample = pd.read_csv(os.path.join(main_path,'sample_submission.csv'))\n",
    "\n",
    "train_df = train_df.drop([1703,1505,379,1173],axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = (768,768,3)\n",
    "EPOCHS = 40\n",
    "SEED = 0\n",
    "#FOLDS = 5\n",
    "TTA = 5\n",
    "#MULTIPLE_OVERSAMPLE_NUM = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
    "print('BATCH_SIZE = {}'.format(BATCH_SIZE))\n",
    "\n",
    "gcs_path = KaggleDatasets().get_gcs_path('plant-pathology-2020-fgvc7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_path(st):\n",
    "    return gcs_path + '/images/' + st + '.jpg'\n",
    "\n",
    "paths = train_df.image_id.apply(format_path).values\n",
    "labels = train_df.loc[:, 'healthy':].values\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(paths, labels, test_size=0.15, random_state=SEED)\n",
    "\n",
    "test_paths = test_df.image_id.apply(format_path).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom ):\n",
    "    \n",
    "    ############## author : chris deotte ##################\n",
    "\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "\n",
    "    zoom_matrix = tf.reshape( tf.concat([height_zoom,zero,zero, zero,width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "       \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), zoom_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_images(filename,label=None,image_size=(DIMS[0],DIMS[1])):\n",
    "    bits = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(bits,channels=3)\n",
    "    img = tf.cast(img,tf.float32)/255.0\n",
    "    img_mean = tf.keras.backend.mean(img)\n",
    "    img_std = tf.keras.backend.std(img)\n",
    "    img -= img_mean\n",
    "    img /= img_std\n",
    "    img = tf.image.resize(img, (DIMS[0], DIMS[1]) )\n",
    "    \n",
    "    if label is None:\n",
    "        return img\n",
    "    else:\n",
    "        return img, label\n",
    "       \n",
    "\n",
    "def augment_data(img,label=None):\n",
    "    \n",
    "    img = tf.image.random_brightness(img, 0.2,seed = SEED)\n",
    "    img = tf.image.random_contrast(img,0.8,1.2,seed = SEED)\n",
    "    #img = tf.image.rot90(img, k = int(np.random.rand()*100) % 4)\n",
    "    img = tf.image.random_flip_up_down(img, seed = SEED)\n",
    "    img = tf.image.random_flip_left_right(img, seed = SEED)\n",
    "    #img = tf.image.resize(img, (DIMS[0], DIMS[1]) )\n",
    "    \n",
    "    if label is None:\n",
    "        return img\n",
    "    else:\n",
    "        return img,label\n",
    "    \n",
    "\n",
    "def transform(image,label=None):\n",
    "    DIM = DIMS[0]\n",
    "    XDIM = DIM % 2 \n",
    "    \n",
    "    rot = 90 * tf.random.uniform(shape=[1])\n",
    "    shr = 20 * tf.random.uniform(shape=[1]) \n",
    "    zoom_factor = tf.random.uniform(shape=[1],minval=0.9, maxval=1.1, seed = SEED)\n",
    "    h_zoom, w_zoom = zoom_factor, zoom_factor\n",
    "\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom) \n",
    "\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "              \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
    "        \n",
    "    if label is None:\n",
    "        return tf.reshape(d,[DIM,DIM,3])\n",
    "    else:\n",
    "        return tf.reshape(d,[DIM,DIM,3]),label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(train_paths,train_labels):\n",
    "    train_dataset = (tf.data.Dataset\n",
    "                    .from_tensor_slices((train_paths, train_labels))\n",
    "                    .map(decode_images, num_parallel_calls=AUTO)\n",
    "                    .map(augment_data, num_parallel_calls=AUTO)\n",
    "                    .shuffle(512)\n",
    "                    #.map(transform, num_parallel_calls=AUTO)\n",
    "                    .batch(BATCH_SIZE)\n",
    "                    .prefetch(AUTO)\n",
    "                    )\n",
    "    return train_dataset\n",
    "\n",
    "def get_validation_dataset(valid_paths,valid_labels):\n",
    "    valid_dataset = (tf.data.Dataset\n",
    "                    .from_tensor_slices((valid_paths, valid_labels))\n",
    "                    .map(decode_images, num_parallel_calls=AUTO)\n",
    "                    .map(augment_data, num_parallel_calls=AUTO)\n",
    "                    .shuffle(512)\n",
    "                    #.map(transform, num_parallel_calls=AUTO)\n",
    "                    .batch(BATCH_SIZE)\n",
    "                    .prefetch(AUTO)\n",
    "                    )\n",
    "    return valid_dataset\n",
    "\n",
    "def get_test_dataset(test_paths):\n",
    "    test_dataset = (tf.data.Dataset\n",
    "                    .from_tensor_slices((test_paths))\n",
    "                    .map(decode_images, num_parallel_calls=AUTO)\n",
    "                    .map(augment_data, num_parallel_calls=AUTO)\n",
    "                    #.map(transform, num_parallel_calls=AUTO)\n",
    "                    .batch(BATCH_SIZE)\n",
    "                    )\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.Sequential([\n",
    "            efn.EfficientNetB7(input_shape=DIMS,\n",
    "                               weights='noisy-student',\n",
    "                               include_top=None),\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss = 'categorical_crossentropy',\n",
    "            metrics=[ tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "                      tf.keras.metrics.AUC(name='auc')\n",
    "                    ]\n",
    "            )\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_START = 0.00001\n",
    "LR_MAX = 0.0001 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.000001\n",
    "LR_RAMPUP_EPOCHS = 10\n",
    "LR_SUSTAIN_EPOCHS = 5\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n",
    "\n",
    "ch_p = ModelCheckpoint(filepath=\"model_ef.h5\", monitor=\"val_loss\", \n",
    "                       save_best_only=True, mode='min',\n",
    "                       save_weights_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "STEPS_PER_EPOCH = labels.shape[0] // BATCH_SIZE\n",
    "total_predictions = []\n",
    "\n",
    "\n",
    "for i in range(FOLDS):\n",
    "    print('-'*30)\n",
    "    print('FOLD NUMBER : {}'.format(i+1))\n",
    "    print('-'*30)\n",
    "    \n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    model = get_model()\n",
    "    \n",
    "    history = model.fit(get_training_dataset(paths,labels), \n",
    "                        epochs = EPOCHS, \n",
    "                        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                        callbacks = [lr_callback, ch_p], \n",
    "                        verbose = 1)\n",
    "    histories.append(history)\n",
    "    \n",
    "    for i in range(TTA):\n",
    "        preds = model.predict(get_test_dataset(test_paths), verbose=1)\n",
    "        total_predictions.append(preds)\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "model = get_model()\n",
    "    \n",
    "history = model.fit(get_training_dataset(train_paths, train_labels), \n",
    "                    epochs = EPOCHS, \n",
    "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    callbacks = [lr_callback, ch_p],\n",
    "                    #validation_data = get_validation_dataset(val_paths,val_labels),\n",
    "                    verbose = 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n",
    "    \"\"\"\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'], \n",
    "    history.history['val_loss'], \n",
    "    'loss', 211)\n",
    "display_training_curves(\n",
    "    history.history['categorical_accuracy'], \n",
    "    history.history['val_categorical_accuracy'], \n",
    "    'accuracy', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predictions =[]\n",
    "model.load_weights('model_ef.h5')\n",
    "print('done')\n",
    "for i in range(1):\n",
    "    preds = model.predict(get_test_dataset(test_paths), verbose=1)\n",
    "    total_predictions.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sum = 0\n",
    "for prob in total_predictions:\n",
    "    prob_sum += prob\n",
    "prob_avg = prob_sum/(1)\n",
    "\n",
    "sample.loc[:, 'healthy':] = prob_avg\n",
    "sample.to_csv('Enetb7_size_600*900.csv', index=False)\n",
    "sample.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in range(10):\n",
    "    preds = model.predict(get_test_dataset(test_paths), verbose = 1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "\n",
    "final_preds=0\n",
    "for preds in predictions:\n",
    "    final_preds += preds\n",
    "final_preds /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.loc[:, 'healthy':] = final_preds\n",
    "sample.to_csv('submission.csv', index=False)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    histories[0].history['loss'], \n",
    "    histories[0].history['val_loss'], \n",
    "    'loss', 211)\n",
    "display_training_curves(\n",
    "    histories[0].history['categorical_accuracy'], \n",
    "    histories[0].history['val_categorical_accuracy'], \n",
    "    'accuracy', 212)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    histories[1].history['loss'], \n",
    "    histories[1].history['val_loss'], \n",
    "    'loss', 211)\n",
    "display_training_curves(\n",
    "    histories[1].history['categorical_accuracy'], \n",
    "    histories[1].history['val_categorical_accuracy'], \n",
    "    'accuracy', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    histories[2].history['loss'], \n",
    "    histories[2].history['val_loss'], \n",
    "    'loss', 211)\n",
    "display_training_curves(\n",
    "    histories[2].history['categorical_accuracy'], \n",
    "    histories[2].history['val_categorical_accuracy'], \n",
    "    'accuracy', 212)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
